"""
Simplified LLM client for metadata extraction.
This is a placeholder that would need to be implemented with your actual LLM provider.
"""
import json
import logging
import os
from google import genai
from google.genai import types
from typing import Optional
from enum import Enum


from src.schemas import PaperMetadataExtraction
from src.utils import retry_llm_operation

logger = logging.getLogger(__name__)

# LLM Prompt for metadata extraction
EXTRACT_PAPER_METADATA = """
You are a metadata extraction assistant. Your task is to extract relevant information from academic papers.

Given the following paper, extract the title, authors, abstract, institutions, keywords, annotations, summary, useful starter questions that can be asked about the paper, and the publish date. The information should be structured in a JSON format.

Extract the title in title case.

Extract the abstract in normal case.

Please provide the necessary details for extraction. Ensure that the information is accurate and complete.

Please format the information in a JSON object as follows:
Schema: {schema}
"""

DEFAULT_CHAT_MODEL = "gemini-2.5-pro-preview-03-25"

class JSONParser:
    @staticmethod
    def validate_and_extract_json(response_text: str) -> dict:
        """Extract and validate JSON from LLM response."""
        # Try to find JSON in the response
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}') + 1

        if start_idx == -1 or end_idx == 0:
            raise ValueError("No JSON found in response")

        json_str = response_text[start_idx:end_idx]

        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON: {e}")

class SimpleLLMClient:
    """
    A simple LLM client for metadata extraction.
    This is a placeholder implementation that would need to be replaced
    with actual LLM API calls (OpenAI, Anthropic, Google, etc.)
    """

    def __init__(
        self,
        api_key: str,
        default_model: Optional[str] = None,
    ):
        self.api_key = api_key
        self.default_model: str = default_model or DEFAULT_CHAT_MODEL
        self.client = genai.Client(api_key=self.api_key)

    def create_cache(self, cache_content: str) -> str:
        """Create a cache entry for the given content.

        Args:
            cache_content (str): The content to cache.

        Returns:
            str: The cache key for the stored content.
        """

        cached_content = self.client.caches.create(
            model=self.default_model,
            config=types.CreateCachedContentConfig(
                contents=types.Content(
                    role='user',
                    parts=[
                        types.Part.from_text(text=cache_content)
                    ]
                ),
                display_name="Paper Metadata Cache",
                ttl='3600s'
            )
        )

        if cached_content and cached_content.name:
            logger.info(f"Cache created successfully: {cached_content.name}")
        else:
            logger.error("Failed to create cache entry")
            raise ValueError("Cache creation failed")

        return cached_content.name

    def generate_content(self, prompt: str, cache_key: Optional[str] = None, model: Optional[str] = None) -> str:
        """
        Generate content using the LLM.

        Args:
            prompt: The prompt to send to the LLM
            model: Optional specific model to use, defaults to self.default_model

        Returns:
            str: The generated content from the LLM
        """
        if not model:
            model = self.default_model

        response = self.client.models.generate_content(
            model=model if model else self.default_model,
            contents=prompt,
            config=types.GenerateContentConfig(
                cached_content=cache_key,
            )
        )

        if response and response.text:
            return response.text

        raise ValueError("No content generated by the LLM")

class PaperOperations(SimpleLLMClient):
    """
    Simplified LLM client for metadata extraction.
    This is a placeholder implementation that would need to be replaced
    with actual LLM API calls (OpenAI, Anthropic, Google, etc.)
    """

    def __init__(self, api_key: str, default_model: Optional[str] = None):
        """Initialize the LLM client for paper operations."""
        super().__init__(api_key, default_model=default_model)

    @retry_llm_operation(max_retries=3, delay=1.0)
    def extract_paper_metadata(self, paper_content: str) -> PaperMetadataExtraction:
        """
        Extract metadata from paper content using LLM.

        Args:
            paper_content: The extracted text content from the PDF

        Returns:
            PaperMetadataExtraction: Extracted metadata
        """
        try:
            # Upload the paper content to cache
            cache_key = self.create_cache(paper_content)

            # Format the prompt
            formatted_prompt = EXTRACT_PAPER_METADATA.format(
                schema=PaperMetadataExtraction.model_json_schema()
            )

            response = self.generate_content(formatted_prompt, cache_key=cache_key)

            # Parse and validate the response
            response_json = JSONParser.validate_and_extract_json(response)

            # Parse the response and return the metadata
            metadata = PaperMetadataExtraction.model_validate(response_json)

            return metadata

        except Exception as e:
            logger.error(f"Error extracting metadata: {e}", exc_info=True)
            # Return minimal metadata if extraction fails
            raise ValueError(f"Failed to extract metadata: {str(e)}")

# Create a single instance to use throughout the application
api_key = os.getenv("GOOGLE_API_KEY")

if not api_key:
    raise ValueError("GOOGLE_API_KEY environment variable is not set")

llm_client = PaperOperations(api_key=api_key, default_model=DEFAULT_CHAT_MODEL)
