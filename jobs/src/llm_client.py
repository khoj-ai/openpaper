"""
Simplified LLM client for metadata extraction.
This is a placeholder that would need to be implemented with your actual LLM provider.
"""
import json
import logging
import os
from openai import OpenAI
from typing import Optional
from enum import Enum

from openai.types.chat import (
    ChatCompletionUserMessageParam,
)


from src.schemas import PaperMetadataExtraction
from src.utils import retry_llm_operation

logger = logging.getLogger(__name__)

# LLM Prompt for metadata extraction
EXTRACT_PAPER_METADATA = """
You are a metadata extraction assistant. Your task is to extract relevant information from academic papers.

Given the following paper, extract the title, authors, abstract, institutions, keywords, annotations, summary, useful starter questions that can be asked about the paper, and the publish date. The information should be structured in a JSON format.

Extract the title in title case.

Extract the abstract in normal case.

Paper: {paper}

Please provide the necessary details for extraction. Ensure that the information is accurate and complete.

Please format the information in a JSON object as follows:
Schema: {schema}
"""

DEFAULT_CHAT_MODEL = "gemini-2.5-pro-preview-03-25"
GEMINI_API_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai/"

class JSONParser:
    @staticmethod
    def validate_and_extract_json(response_text: str) -> dict:
        """Extract and validate JSON from LLM response."""
        # Try to find JSON in the response
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}') + 1

        if start_idx == -1 or end_idx == 0:
            raise ValueError("No JSON found in response")

        json_str = response_text[start_idx:end_idx]

        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON: {e}")

class LLMProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"

    @classmethod
    def from_string(cls, provider_str: str) -> 'LLMProvider':
        """Convert string to LLMProvider enum."""
        try:
            return cls[provider_str.upper()]
        except KeyError:
            raise ValueError(f"Unknown LLM provider: {provider_str}")

class SimpleLLMClient:
    """
    A simple LLM client for metadata extraction.
    This is a placeholder implementation that would need to be replaced
    with actual LLM API calls (OpenAI, Anthropic, Google, etc.)
    """

    def __init__(
        self,
        api_key: str,
        default_model: Optional[str] = None,
        llm_provider: LLMProvider = LLMProvider.GOOGLE,
    ):
        self.api_key = api_key
        self.default_model: str = default_model or DEFAULT_CHAT_MODEL
        self.llm_provider = llm_provider
        base_url = GEMINI_API_BASE_URL if llm_provider == LLMProvider.GOOGLE else None
        self.client = OpenAI(api_key=self.api_key, base_url=base_url)

    def generate_content(self, prompt: str, model: Optional[str] = None) -> str:
        """
        Generate content using the LLM.

        Args:
            prompt: The prompt to send to the LLM
            model: Optional specific model to use, defaults to self.default_model

        Returns:
            str: The generated content from the LLM
        """
        if not model:
            model = self.default_model

        user_msg: ChatCompletionUserMessageParam = {
            "role": "user",
            "content": prompt,
        }

        response = self.client.chat.completions.create(
            model=model if model else self.default_model,
            messages=[user_msg]
        )

        if not response.choices or not response.choices[0].message.content:
            raise ValueError("No content generated by the LLM")

        return response.choices[0].message.content.strip()


class PaperOperations(SimpleLLMClient):
    """
    Simplified LLM client for metadata extraction.
    This is a placeholder implementation that would need to be replaced
    with actual LLM API calls (OpenAI, Anthropic, Google, etc.)
    """

    def __init__(self, api_key: str, default_model: Optional[str] = None, llm_provider: LLMProvider = LLMProvider.GOOGLE):
        """Initialize the LLM client for paper operations."""
        super().__init__(api_key, default_model=default_model, llm_provider=llm_provider)

    @retry_llm_operation(max_retries=3, delay=1.0)
    def extract_paper_metadata(self, paper_content: str) -> PaperMetadataExtraction:
        """
        Extract metadata from paper content using LLM.

        Args:
            paper_content: The extracted text content from the PDF

        Returns:
            PaperMetadataExtraction: Extracted metadata
        """
        try:
            # Format the prompt
            formatted_prompt = EXTRACT_PAPER_METADATA.format(
                paper=paper_content,
                schema=PaperMetadataExtraction.model_json_schema()
            )

            response = self.generate_content(formatted_prompt)

            # Parse and validate the response
            response_json = JSONParser.validate_and_extract_json(response)

            # Parse the response and return the metadata
            metadata = PaperMetadataExtraction.model_validate(response_json)

            return metadata

        except Exception as e:
            logger.error(f"Error extracting metadata: {e}", exc_info=True)
            # Return minimal metadata if extraction fails
            raise ValueError(f"Failed to extract metadata: {str(e)}")

# Create a single instance to use throughout the application
api_key = os.getenv("GOOGLE_API_KEY")

if not api_key:
    raise ValueError("GOOGLE_API_KEY environment variable is not set")

llm_client = PaperOperations(api_key=api_key, default_model=DEFAULT_CHAT_MODEL, llm_provider=LLMProvider.GOOGLE)
